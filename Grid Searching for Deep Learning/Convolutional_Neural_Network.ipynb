{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapting the framework to grid search CNN models.\n",
    "Much the same set of hyperparameters can be searched as with the MLP model, except the number of nodes in the hidden layer can be replaced by the number of filter maps and kernel size in the convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chosen set of hyperparameters to grid search in the CNN model:\n",
    "\n",
    "**n_input**: The number of prior inputs to use as input for the model (e.g. 12 months).\n",
    "\n",
    "**n_filters**: The number of filter maps in the convolutional layer (e.g. 32).\n",
    "\n",
    "**n_kernel**: The kernel size in the convolutional layer (e.g. 3).\n",
    "\n",
    "**n_epochs**: The number of training epochs (e.g. 1000).\n",
    "\n",
    "**n_batch**: The number of samples to include in each mini-batch (e.g. 32).\n",
    "\n",
    "**n_diff**: The difference order (e.g. 0 or 12).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Some additional hyperparameters that may be investigated are:\n",
    "the use of two convolutional layers before a pooling layer, the repetition of the convolutional and pooling layer pattern, the use of dropout, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We will define a very simple CNN model with one convolutional layer and one pooling layer.*\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=n_filters, kernel_size=n_kernel, activation='relu', input_shape=(n_input, n_features)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data must be prepared in much the same way as for the MLP.\n",
    "\n",
    "Unlike the MLP that expects the input data to have the shape [samples, features], the 1D CNN model expects the data to have the shape [samples, timesteps, features] where features maps onto channels and in this case 1 for the one variable we measure each month."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# reshape input data into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "train_x = train_x.reshape((train_x.shape[0], train_x.shape[1], n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Complete implementation of model_fit() function :*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model\n",
    "def model_fit(train, config):\n",
    "\t# unpack config\n",
    "\tn_input, n_filters, n_kernel, n_epochs, n_batch, n_diff = config\n",
    "\t# prepare data\n",
    "\tif n_diff > 0:\n",
    "\t\ttrain = difference(train, n_diff)\n",
    "\t# transform series into supervised format\n",
    "\tdata = series_to_supervised(train, n_in=n_input)\n",
    "\t# separate inputs and outputs\n",
    "\ttrain_x, train_y = data[:, :-1], data[:, -1]\n",
    "\t# reshape input data into [samples, timesteps, features]\n",
    "\tn_features = 1\n",
    "\ttrain_x = train_x.reshape((train_x.shape[0], train_x.shape[1], n_features))\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv1D(filters=n_filters, kernel_size=n_kernel, activation='relu', input_shape=(n_input, n_features)))\n",
    "\tmodel.add(MaxPooling1D(pool_size=2))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(1))\n",
    "\tmodel.compile(loss='mse', optimizer='adam')\n",
    "\t# fit\n",
    "\tmodel.fit(train_x, train_y, epochs=n_epochs, batch_size=n_batch, verbose=0)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a prediction with a CNN is almost the same as MLP.\n",
    "\n",
    "Again, the only difference is that the one sample worth of input data must have a three-dimensional shape."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_input = array(history[-n_input:]).reshape((1, n_input, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Complete implementation of model_predict() function :*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast with the fit model\n",
    "def model_predict(model, history, config):\n",
    "\t# unpack config\n",
    "\tn_input, _, _, _, _, n_diff = config\n",
    "\t# prepare data\n",
    "\tcorrection = 0.0\n",
    "\tif n_diff > 0:\n",
    "\t\tcorrection = history[-n_diff]\n",
    "\t\thistory = difference(history, n_diff)\n",
    "\tx_input = array(history[-n_input:]).reshape((1, n_input, 1))\n",
    "\t# forecast\n",
    "\tyhat = model.predict(x_input, verbose=0)\n",
    "\treturn correction + yhat[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can define a list of configurations for the model to evaluate. As before, we can do this by defining lists of hyperparameter values to try that are combined into a list. We will try a small number of configurations to ensure the example executes in a reasonable amount of time.\n",
    "\n",
    "*The complete model_configs() function:*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of configs to try\n",
    "def model_configs():\n",
    "\t# define scope of configs\n",
    "\tn_input = [12]\n",
    "\tn_filters = [64]\n",
    "\tn_kernels = [3, 5]\n",
    "\tn_epochs = [100]\n",
    "\tn_batch = [1, 150]\n",
    "\tn_diff = [0, 12]\n",
    "\t# create configs\n",
    "\tconfigs = list()\n",
    "\tfor a in n_input:\n",
    "\t\tfor b in n_filters:\n",
    "\t\t\tfor c in n_kernels:\n",
    "\t\t\t\tfor d in n_epochs:\n",
    "\t\t\t\t\tfor e in n_batch:\n",
    "\t\t\t\t\t\tfor f in n_diff:\n",
    "\t\t\t\t\t\t\tcfg = [a,b,c,d,e,f]\n",
    "\t\t\t\t\t\t\tconfigs.append(cfg)\n",
    "\tprint('Total configs: %d' % len(configs))\n",
    "\treturn configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combining all the elements needed to grid search the hyperparameters of a convolutional neural network for univariate time series forecasting: **\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total configs: 8\n",
      " > 20.393\n",
      " > 17.763\n",
      " > 18.252\n",
      " > 13.629\n",
      " > 23.648\n",
      " > 26.389\n",
      " > 21.445\n",
      " > 16.613\n",
      " > 18.027\n",
      " > 21.254\n",
      "> Model[[12, 64, 3, 100, 1, 0]] 19.741\n",
      " > 20.373\n",
      " > 19.875\n",
      " > 19.478\n",
      " > 19.386\n",
      " > 19.904\n",
      " > 19.525\n",
      " > 19.683\n",
      " > 20.750\n",
      " > 19.428\n",
      " > 21.227\n",
      "> Model[[12, 64, 3, 100, 1, 12]] 19.963\n",
      " > 69.863\n",
      " > 72.446\n",
      " > 74.578\n",
      " > 78.899\n",
      " > 74.200\n",
      " > 83.236\n",
      " > 74.781\n",
      " > 81.508\n",
      " > 83.510\n",
      " > 81.655\n",
      "> Model[[12, 64, 3, 100, 150, 0]] 77.468\n",
      " > 19.026\n",
      " > 18.983\n",
      " > 18.730\n",
      " > 18.986\n",
      " > 18.333\n",
      " > 19.854\n",
      " > 18.199\n",
      " > 20.089\n",
      " > 18.579\n",
      " > 18.792\n",
      "> Model[[12, 64, 3, 100, 150, 12]] 18.957\n",
      " > 27.150\n",
      " > 21.661\n",
      " > 23.156\n",
      " > 21.136\n",
      " > 16.542\n",
      " > 17.664\n",
      " > 23.652\n",
      " > 22.838\n",
      " > 23.584\n",
      " > 21.707\n",
      "> Model[[12, 64, 5, 100, 1, 0]] 21.909\n",
      " > 19.787\n",
      " > 19.177\n",
      " > 20.075\n",
      " > 22.892\n",
      " > 19.694\n",
      " > 18.455\n",
      " > 20.488\n",
      " > 18.810\n",
      " > 19.870\n",
      " > 18.181\n",
      "> Model[[12, 64, 5, 100, 1, 12]] 19.743\n",
      " > 84.151\n",
      " > 76.320\n",
      " > 81.535\n",
      " > 79.452\n",
      " > 77.579\n",
      " > 87.520\n",
      " > 86.215\n",
      " > 84.669\n",
      " > 92.715\n",
      " > 84.164\n",
      "> Model[[12, 64, 5, 100, 150, 0]] 83.432\n",
      " > 20.489\n",
      " > 19.238\n",
      " > 20.603\n",
      " > 19.122\n",
      " > 19.450\n",
      " > 18.897\n",
      " > 20.515\n",
      " > 19.944\n",
      " > 19.184\n",
      " > 18.918\n",
      "> Model[[12, 64, 5, 100, 150, 12]] 19.636\n",
      "done\n",
      "[12, 64, 3, 100, 150, 12] 18.957192866441392\n",
      "[12, 64, 5, 100, 150, 12] 19.635966808420275\n",
      "[12, 64, 3, 100, 1, 0] 19.74137720728019\n"
     ]
    }
   ],
   "source": [
    "# grid search cnn for airline passengers\n",
    "from math import sqrt\n",
    "from numpy import array\n",
    "from numpy import mean\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def train_test_split(data, n_test):\n",
    "\treturn data[:-n_test], data[-n_test:]\n",
    "\n",
    "# transform list into supervised learning format\n",
    "def series_to_supervised(data, n_in=1, n_out=1):\n",
    "\tdf = DataFrame(data)\n",
    "\tcols = list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\t# drop rows with NaN values\n",
    "\tagg.dropna(inplace=True)\n",
    "\treturn agg.values\n",
    "\n",
    "# root mean squared error or rmse\n",
    "def measure_rmse(actual, predicted):\n",
    "\treturn sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "# difference dataset\n",
    "def difference(data, order):\n",
    "\treturn [data[i] - data[i - order] for i in range(order, len(data))]\n",
    "\n",
    "# fit a model\n",
    "def model_fit(train, config):\n",
    "\t# unpack config\n",
    "\tn_input, n_filters, n_kernel, n_epochs, n_batch, n_diff = config\n",
    "\t# prepare data\n",
    "\tif n_diff > 0:\n",
    "\t\ttrain = difference(train, n_diff)\n",
    "\t# transform series into supervised format\n",
    "\tdata = series_to_supervised(train, n_in=n_input)\n",
    "\t# separate inputs and outputs\n",
    "\ttrain_x, train_y = data[:, :-1], data[:, -1]\n",
    "\t# reshape input data into [samples, timesteps, features]\n",
    "\tn_features = 1\n",
    "\ttrain_x = train_x.reshape((train_x.shape[0], train_x.shape[1], n_features))\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv1D(filters=n_filters, kernel_size=n_kernel, activation='relu', input_shape=(n_input, n_features)))\n",
    "\tmodel.add(MaxPooling1D(pool_size=2))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(1))\n",
    "\tmodel.compile(loss='mse', optimizer='adam')\n",
    "\t# fit\n",
    "\tmodel.fit(train_x, train_y, epochs=n_epochs, batch_size=n_batch, verbose=0)\n",
    "\treturn model\n",
    "\n",
    "# forecast with the fit model\n",
    "def model_predict(model, history, config):\n",
    "\t# unpack config\n",
    "\tn_input, _, _, _, _, n_diff = config\n",
    "\t# prepare data\n",
    "\tcorrection = 0.0\n",
    "\tif n_diff > 0:\n",
    "\t\tcorrection = history[-n_diff]\n",
    "\t\thistory = difference(history, n_diff)\n",
    "\tx_input = array(history[-n_input:]).reshape((1, n_input, 1))\n",
    "\t# forecast\n",
    "\tyhat = model.predict(x_input, verbose=0)\n",
    "\treturn correction + yhat[0]\n",
    "\n",
    "# walk-forward validation for univariate data\n",
    "def walk_forward_validation(data, n_test, cfg):\n",
    "\tpredictions = list()\n",
    "\t# split dataset\n",
    "\ttrain, test = train_test_split(data, n_test)\n",
    "\t# fit model\n",
    "\tmodel = model_fit(train, cfg)\n",
    "\t# seed history with training dataset\n",
    "\thistory = [x for x in train]\n",
    "\t# step over each time-step in the test set\n",
    "\tfor i in range(len(test)):\n",
    "\t\t# fit model and make forecast for history\n",
    "\t\tyhat = model_predict(model, history, cfg)\n",
    "\t\t# store forecast in list of predictions\n",
    "\t\tpredictions.append(yhat)\n",
    "\t\t# add actual observation to history for the next loop\n",
    "\t\thistory.append(test[i])\n",
    "\t# estimate prediction error\n",
    "\terror = measure_rmse(test, predictions)\n",
    "\tprint(' > %.3f' % error)\n",
    "\treturn error\n",
    "\n",
    "# score a model, return None on failure\n",
    "def repeat_evaluate(data, config, n_test, n_repeats=10):\n",
    "\t# convert config to a key\n",
    "\tkey = str(config)\n",
    "\t# fit and evaluate the model n times\n",
    "\tscores = [walk_forward_validation(data, n_test, config) for _ in range(n_repeats)]\n",
    "\t# summarize score\n",
    "\tresult = mean(scores)\n",
    "\tprint('> Model[%s] %.3f' % (key, result))\n",
    "\treturn (key, result)\n",
    "\n",
    "# grid search configs\n",
    "def grid_search(data, cfg_list, n_test):\n",
    "\t# evaluate configs\n",
    "\tscores = [repeat_evaluate(data, cfg, n_test) for cfg in cfg_list]\n",
    "\t# sort configs by error, asc\n",
    "\tscores.sort(key=lambda tup: tup[1])\n",
    "\treturn scores\n",
    "\n",
    "# create a list of configs to try\n",
    "def model_configs():\n",
    "\t# define scope of configs\n",
    "\tn_input = [12]\n",
    "\tn_filters = [64]\n",
    "\tn_kernels = [3, 5]\n",
    "\tn_epochs = [100]\n",
    "\tn_batch = [1, 150]\n",
    "\tn_diff = [0, 12]\n",
    "\t# create configs\n",
    "\tconfigs = list()\n",
    "\tfor a in n_input:\n",
    "\t\tfor b in n_filters:\n",
    "\t\t\tfor c in n_kernels:\n",
    "\t\t\t\tfor d in n_epochs:\n",
    "\t\t\t\t\tfor e in n_batch:\n",
    "\t\t\t\t\t\tfor f in n_diff:\n",
    "\t\t\t\t\t\t\tcfg = [a,b,c,d,e,f]\n",
    "\t\t\t\t\t\t\tconfigs.append(cfg)\n",
    "\tprint('Total configs: %d' % len(configs))\n",
    "\treturn configs\n",
    "\n",
    "# define dataset\n",
    "series = read_csv('monthly-airline-passengers.csv', header=0, index_col=0)\n",
    "data = series.values\n",
    "# data split\n",
    "n_test = 12\n",
    "# model configs\n",
    "cfg_list = model_configs()\n",
    "# grid search\n",
    "scores = grid_search(data, cfg_list, n_test)\n",
    "print('done')\n",
    "# list top 10 configs\n",
    "for cfg, error in scores[:3]:\n",
    "\tprint(cfg, error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

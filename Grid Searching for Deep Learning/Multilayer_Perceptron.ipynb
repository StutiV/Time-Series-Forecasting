{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a very simple model with one hidden layer and define five hyperparameters to tune. They are:\n",
    "\n",
    "**n_input**: The number of prior inputs to use as input for the model (e.g. 12 months).\n",
    "\n",
    "**n_nodes**: The number of nodes to use in the hidden layer (e.g. 50).\n",
    "\n",
    "**n_epochs**: The number of training epochs (e.g. 1000).\n",
    "\n",
    "**n_batch**: The number of samples to include in each mini-batch (e.g. 32).\n",
    "\n",
    "**n_diff**: The difference order (e.g. 0 or 12).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modern neural networks can handle raw data with little pre-processing, such as scaling and differencing. Nevertheless, when it comes to time series data, sometimes differencing the series can make a problem easier to model.\n",
    "\n",
    "*Differencing is the transform of the data such that a value of a prior observation is subtracted from the current observation, removing trend or seasonality structure.*\n",
    "\n",
    "We will add support for differencing to the grid search test harness, just in case it adds value to a specific problem. It does add value for the internal airline passengers dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference dataset\n",
    "# will calculate the difference of a given order for the dataset\n",
    "def difference(data, order):\n",
    "\treturn [data[i] - data[i - order] for i in range(order, len(data))]\n",
    "\n",
    "#Differencing will be optional, where an order of 0 suggests no differencing, whereas \n",
    "#an order 1 or order 12 will require that the data be differenced prior to fitting the model \n",
    "#and that the predictions of the model will need the differencing reversed prior to returning the forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, unpacking the list of hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack config\n",
    "# n_input, n_nodes, n_epochs, n_batch, n_diff = config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we must prepare the data, including the differencing, transforming the data to a supervised format and separating out the input and output aspects of the data samples"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# prepare data\n",
    "\"\"\"\n",
    "if n_diff > 0:\n",
    "\ttrain = difference(train, n_diff)\n",
    "# transform series into supervised format\n",
    "data = series_to_supervised(train, n_in=n_input)\n",
    "# separate inputs and outputs\n",
    "train_x, train_y = data[:, :-1], data[:, -1]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define and fit the model with the given conifguration."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(n_nodes, activation='relu', input_dim=n_input))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "# fit model\n",
    "model.fit(train_x, train_y, epochs=n_epochs, batch_size=n_batch, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete implementation of the *model_fit()* function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model\n",
    "def model_fit(train, config):\n",
    "\t# unpack config\n",
    "\tn_input, n_nodes, n_epochs, n_batch, n_diff = config\n",
    "\t# prepare data\n",
    "\tif n_diff > 0:\n",
    "\t\ttrain = difference(train, n_diff)\n",
    "\t# transform series into supervised format\n",
    "\tdata = series_to_supervised(train, n_in=n_input)\n",
    "\t# separate inputs and outputs\n",
    "\ttrain_x, train_y = data[:, :-1], data[:, -1]\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(n_nodes, activation='relu', input_dim=n_input))\n",
    "\tmodel.add(Dense(1))\n",
    "\tmodel.compile(loss='mse', optimizer='adam')\n",
    "\t# fit model\n",
    "\tmodel.fit(train_x, train_y, epochs=n_epochs, batch_size=n_batch, verbose=0)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is fit, it can be used for making forecasts.\n",
    "If the data was differenced, the difference must be inverted for the prediction of the model. This involves adding the value at the relative offset from the history back to the value predicted by the model."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# invert difference\n",
    "correction = 0.0\n",
    "if n_diff > 0:\n",
    "\tcorrection = history[-n_diff]\n",
    "...\n",
    "# correct forecast if it was differenced\n",
    "return correction + yhat[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also means that the history must be differenced so that the input data used to make the prediction has the expected form."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# calculate difference\n",
    "history = difference(history, n_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once prepared, we can use the history data to create a single sample as input to the model for making a one-step prediction.\n",
    "\n",
    "The shape of one sample must be [1, n_input] where n_input is the chosen number of lag observations to use."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# shape input for model\n",
    "x_input = array(history[-n_input:]).reshape((1, n_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a prediction can be made."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# make forecast\n",
    "yhat = model.predict(x_input, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete implementation of the model_predict function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast with the fit model\n",
    "def model_predict(model, history, config):\n",
    "\t# unpack config\n",
    "\tn_input, _, _, _, n_diff = config\n",
    "\t# prepare data\n",
    "\tcorrection = 0.0\n",
    "\tif n_diff > 0:\n",
    "\t\tcorrection = history[-n_diff]\n",
    "\t\thistory = difference(history, n_diff)\n",
    "\t# shape input for model\n",
    "\tx_input = array(history[-n_input:]).reshape((1, n_input))\n",
    "\t# make forecast\n",
    "\tyhat = model.predict(x_input, verbose=0)\n",
    "\t# correct forecast if it was differenced\n",
    "\treturn correction + yhat[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a model_configs() function that creates a list of the different combinations of parameters to try.\n",
    "\n",
    "We will define a small subset of configurations to try as an example, including a differencing of 12 months, which we expect will be required.\n",
    "\n",
    "The grid search can be repeated to narrow in on ranges of values that appear to show better performance.\n",
    "\n",
    "An implementation of the model_configs():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of configs to try\n",
    "def model_configs():\n",
    "\t# define scope of configs\n",
    "\tn_input = [12]\n",
    "\tn_nodes = [50, 100]\n",
    "\tn_epochs = [100]\n",
    "\tn_batch = [1, 150]\n",
    "\tn_diff = [0, 12]\n",
    "\t# create configs\n",
    "\tconfigs = list()\n",
    "\tfor i in n_input:\n",
    "\t\tfor j in n_nodes:\n",
    "\t\t\tfor k in n_epochs:\n",
    "\t\t\t\tfor l in n_batch:\n",
    "\t\t\t\t\tfor m in n_diff:\n",
    "\t\t\t\t\t\tcfg = [i, j, k, l, m]\n",
    "\t\t\t\t\t\tconfigs.append(cfg)\n",
    "\tprint('Total configs: %d' % len(configs))\n",
    "\treturn configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining all the elements required to grid search MLP models for a univariate time series forecasting problem, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total configs: 8\n",
      " > 23.091\n",
      " > 17.128\n",
      " > 21.231\n",
      " > 29.980\n",
      " > 18.841\n",
      " > 18.143\n",
      " > 29.214\n",
      " > 21.461\n",
      " > 17.444\n",
      " > 33.589\n",
      "> Model[[12, 50, 100, 1, 0]] 23.012\n",
      " > 20.531\n",
      " > 19.559\n",
      " > 21.614\n",
      " > 19.118\n",
      " > 19.001\n",
      " > 18.362\n",
      " > 21.848\n",
      " > 18.842\n",
      " > 20.018\n",
      " > 20.605\n",
      "> Model[[12, 50, 100, 1, 12]] 19.950\n",
      " > 63.015\n",
      " > 42.279\n",
      " > 91.787\n",
      " > 64.172\n",
      " > 51.806\n",
      " > 96.806\n",
      " > 69.926\n",
      " > 39.364\n",
      " > 36.523\n",
      " > 45.846\n",
      "> Model[[12, 50, 100, 150, 0]] 60.152\n",
      " > 19.593\n",
      " > 20.129\n",
      " > 18.897\n",
      " > 17.921\n",
      " > 20.572\n",
      " > 19.789\n",
      " > 19.861\n",
      " > 19.539\n",
      " > 20.640\n",
      " > 19.409\n",
      "> Model[[12, 50, 100, 150, 12]] 19.635\n",
      " > 17.297\n",
      " > 16.129\n",
      " > 25.786\n",
      " > 19.816\n",
      " > 16.393\n",
      " > 15.813\n",
      " > 21.420\n",
      " > 19.012\n",
      " > 18.209\n",
      " > 20.256\n",
      "> Model[[12, 100, 100, 1, 0]] 19.013\n",
      " > 20.455\n",
      " > 19.656\n",
      " > 19.982\n",
      " > 20.715\n",
      " > 18.962\n",
      " > 20.510\n",
      " > 18.708\n",
      " > 21.391\n",
      " > 19.751\n",
      " > 18.317\n",
      "> Model[[12, 100, 100, 1, 12]] 19.845\n",
      " > 54.982\n",
      " > 23.449\n",
      " > 69.765\n",
      " > 65.526\n",
      " > 57.286\n",
      " > 69.115\n",
      " > 64.826\n",
      " > 48.573\n",
      " > 73.917\n",
      " > 79.937\n",
      "> Model[[12, 100, 100, 150, 0]] 60.738\n",
      " > 19.423\n",
      " > 18.275\n",
      " > 18.635\n",
      " > 20.002\n",
      " > 20.146\n",
      " > 18.702\n",
      " > 19.262\n",
      " > 19.664\n",
      " > 20.575\n",
      " > 19.104\n",
      "> Model[[12, 100, 100, 150, 12]] 19.379\n",
      "done\n",
      "[12, 100, 100, 1, 0] 19.01296228667071\n",
      "[12, 100, 100, 150, 12] 19.37882969205016\n",
      "[12, 50, 100, 150, 12] 19.63514917909641\n"
     ]
    }
   ],
   "source": [
    "# grid search mlps for airline passengers\n",
    "from math import sqrt\n",
    "from numpy import array\n",
    "from numpy import mean\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def train_test_split(data, n_test):\n",
    "\treturn data[:-n_test], data[-n_test:]\n",
    "\n",
    "# transform list into supervised learning format\n",
    "def series_to_supervised(data, n_in=1, n_out=1):\n",
    "\tdf = DataFrame(data)\n",
    "\tcols = list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\t# drop rows with NaN values\n",
    "\tagg.dropna(inplace=True)\n",
    "\treturn agg.values\n",
    "\n",
    "# root mean squared error or rmse\n",
    "def measure_rmse(actual, predicted):\n",
    "\treturn sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "# difference dataset\n",
    "def difference(data, order):\n",
    "\treturn [data[i] - data[i - order] for i in range(order, len(data))]\n",
    "\n",
    "# fit a model\n",
    "def model_fit(train, config):\n",
    "\t# unpack config\n",
    "\tn_input, n_nodes, n_epochs, n_batch, n_diff = config\n",
    "\t# prepare data\n",
    "\tif n_diff > 0:\n",
    "\t\ttrain = difference(train, n_diff)\n",
    "\t# transform series into supervised format\n",
    "\tdata = series_to_supervised(train, n_in=n_input)\n",
    "\t# separate inputs and outputs\n",
    "\ttrain_x, train_y = data[:, :-1], data[:, -1]\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(n_nodes, activation='relu', input_dim=n_input))\n",
    "\tmodel.add(Dense(1))\n",
    "\tmodel.compile(loss='mse', optimizer='adam')\n",
    "\t# fit model\n",
    "\tmodel.fit(train_x, train_y, epochs=n_epochs, batch_size=n_batch, verbose=0)\n",
    "\treturn model\n",
    "\n",
    "# forecast with the fit model\n",
    "def model_predict(model, history, config):\n",
    "\t# unpack config\n",
    "\tn_input, _, _, _, n_diff = config\n",
    "\t# prepare data\n",
    "\tcorrection = 0.0\n",
    "\tif n_diff > 0:\n",
    "\t\tcorrection = history[-n_diff]\n",
    "\t\thistory = difference(history, n_diff)\n",
    "\t# shape input for model\n",
    "\tx_input = array(history[-n_input:]).reshape((1, n_input))\n",
    "\t# make forecast\n",
    "\tyhat = model.predict(x_input, verbose=0)\n",
    "\t# correct forecast if it was differenced\n",
    "\treturn correction + yhat[0]\n",
    "\n",
    "# walk-forward validation for univariate data\n",
    "def walk_forward_validation(data, n_test, cfg):\n",
    "\tpredictions = list()\n",
    "\t# split dataset\n",
    "\ttrain, test = train_test_split(data, n_test)\n",
    "\t# fit model\n",
    "\tmodel = model_fit(train, cfg)\n",
    "\t# seed history with training dataset\n",
    "\thistory = [x for x in train]\n",
    "\t# step over each time-step in the test set\n",
    "\tfor i in range(len(test)):\n",
    "\t\t# fit model and make forecast for history\n",
    "\t\tyhat = model_predict(model, history, cfg)\n",
    "\t\t# store forecast in list of predictions\n",
    "\t\tpredictions.append(yhat)\n",
    "\t\t# add actual observation to history for the next loop\n",
    "\t\thistory.append(test[i])\n",
    "\t# estimate prediction error\n",
    "\terror = measure_rmse(test, predictions)\n",
    "\tprint(' > %.3f' % error)\n",
    "\treturn error\n",
    "\n",
    "# score a model, return None on failure\n",
    "def repeat_evaluate(data, config, n_test, n_repeats=10):\n",
    "\t# convert config to a key\n",
    "\tkey = str(config)\n",
    "\t# fit and evaluate the model n times\n",
    "\tscores = [walk_forward_validation(data, n_test, config) for _ in range(n_repeats)]\n",
    "\t# summarize score\n",
    "\tresult = mean(scores)\n",
    "\tprint('> Model[%s] %.3f' % (key, result))\n",
    "\treturn (key, result)\n",
    "\n",
    "# grid search configs\n",
    "def grid_search(data, cfg_list, n_test):\n",
    "\t# evaluate configs\n",
    "\tscores = [repeat_evaluate(data, cfg, n_test) for cfg in cfg_list]\n",
    "\t# sort configs by error, asc\n",
    "\tscores.sort(key=lambda tup: tup[1])\n",
    "\treturn scores\n",
    "\n",
    "# create a list of configs to try\n",
    "def model_configs():\n",
    "\t# define scope of configs\n",
    "\tn_input = [12]\n",
    "\tn_nodes = [50, 100]\n",
    "\tn_epochs = [100]\n",
    "\tn_batch = [1, 150]\n",
    "\tn_diff = [0, 12]\n",
    "\t# create configs\n",
    "\tconfigs = list()\n",
    "\tfor i in n_input:\n",
    "\t\tfor j in n_nodes:\n",
    "\t\t\tfor k in n_epochs:\n",
    "\t\t\t\tfor l in n_batch:\n",
    "\t\t\t\t\tfor m in n_diff:\n",
    "\t\t\t\t\t\tcfg = [i, j, k, l, m]\n",
    "\t\t\t\t\t\tconfigs.append(cfg)\n",
    "\tprint('Total configs: %d' % len(configs))\n",
    "\treturn configs\n",
    "\n",
    "# define dataset\n",
    "series = read_csv('monthly-airline-passengers.csv', header=0, index_col=0)\n",
    "data = series.values\n",
    "# data split\n",
    "n_test = 12\n",
    "# model configs\n",
    "cfg_list = model_configs()\n",
    "# grid search\n",
    "scores = grid_search(data, cfg_list, n_test)\n",
    "print('done')\n",
    "# list top 3 configs\n",
    "for cfg, error in scores[:3]:\n",
    "\tprint(cfg, error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example, we can see that there are a total of eight configurations to be evaluated by the framework.\n",
    "\n",
    "Results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. We'll run the example a few times and compare the average outcome.\n",
    "\n",
    "Each config will be evaluated 10 times; that means 10 models will be created and evaluated using walk-forward validation to calculate an RMSE score before an average of those 10 scores is reported and used to score the configuration.\n",
    "\n",
    "The scores are then sorted and the top 3 configurations with the lowest RMSE are reported at the end. A skillful model configuration was found as compared to a naive model that reported an RMSE of 50.70."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
